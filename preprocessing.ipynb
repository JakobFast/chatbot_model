import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer

# Load data into a pandas dataframe
data = pd.read_csv('example_data.csv')

# Convert text to lowercase
data['question'] = data['question'].str.lower()
data['response'] = data['response'].str.lower()

# Tokenize the text
data['question_tokens'] = data['question'].apply(lambda x: word_tokenize(x))
data['response_tokens'] = data['response'].apply(lambda x: word_tokenize(x))

# Remove stop words
stop_words = set(stopwords.words('english'))
data['question_tokens'] = data['question_tokens'].apply(lambda x: [word for word in x if word not in stop_words])
data['response_tokens'] = data['response_tokens'].apply(lambda x: [word for word in x if word not in stop_words])

# Perform stemming
ps = PorterStemmer()
data['question_tokens'] = data['question_tokens'].apply(lambda x: [ps.stem(word) for word in x])
data['response_tokens'] = data['response_tokens'].apply(lambda x: [ps.stem(word) for word in x])

# Perform lemmatization
lemmatizer = WordNetLemmatizer()
data['question_tokens'] = data['question_tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])
data['response_tokens'] = data['response_tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])
